import httpx
from typing import Optional, Dict, List
from utils.vector_store import search_tramites

OLLAMA_URL = "http://nodo-ia:11434/api/generate"
OLLAMA_MODEL = "llama3.2:3b"

def formatear_tramite_como_texto(tramite: Dict) -> str:
    """
    Convierte un tr√°mite (JSON) a texto estructurado y legible para el LLM
    
    Args:
        tramite: Diccionario con la estructura JSON del tr√°mite
    
    Returns:
        str: Texto formateado con toda la informaci√≥n del tr√°mite
    """
    texto = f"""TR√ÅMITE: {tramite['titulo']}

DESCRIPCI√ìN:
{tramite['descripcion']}

¬øQUI√âN PUEDE REALIZAR EL TR√ÅMITE?
{tramite['quien_puede_realizar']['texto']}"""
    
    if tramite['quien_puede_realizar']['enlaces']:
        texto += f"\nEnlaces √∫tiles: {', '.join(tramite['quien_puede_realizar']['enlaces'])}"
    
    if tramite['documentacion_necesaria']['items']:
        texto += "\n\nDOCUMENTACI√ìN NECESARIA:"
        for item in tramite['documentacion_necesaria']['items']:
            texto += f"\n- {item}"
        
        if tramite['documentacion_necesaria']['enlaces']:
            texto += f"\nEnlaces √∫tiles: {', '.join(tramite['documentacion_necesaria']['enlaces'])}"
    
    texto += f"\n\n¬øD√ìNDE REALIZAR ESTE TR√ÅMITE?\n{tramite['donde_realizar']['texto']}"
    
    if tramite['donde_realizar']['enlaces']:
        texto += f"\nEnlaces √∫tiles: {', '.join(tramite['donde_realizar']['enlaces'])}"
    
    texto += f"\n\nURL OFICIAL: {tramite['url_oficial']}"
    
    return texto

def construir_prompt_con_contexto(
    consulta: str, 
    contexto: str, 
    nombre_usuario: str,
    historial: str = ""
) -> str:
    system_prompt = f"""Eres un asistente especializado en tr√°mites de PAMI (Programa de Atenci√≥n M√©dica Integral).

El usuario se llama {nombre_usuario}. Dirigite a √©l/ella por su nombre cuando sea apropiado.

REGLAS ESTRICTAS Y PRIORITARIAS:
1. Tu respuesta debe basarse EXCLUSIVAMENTE en el CONTEXTO DEL TR√ÅMITE ACTUAL que se muestra abajo
2. Cuando listen documentos, mencion√° TODOS los items sin omitir ninguno
3. NO uses informaci√≥n de mensajes anteriores si contradice o no aparece en el CONTEXTO ACTUAL
4. NO inventes procedimientos, n√∫meros de tel√©fono o detalles que no est√©n expl√≠citos en el CONTEXTO ACTUAL
5. Si el usuario pregunta algo que NO est√° en el CONTEXTO ACTUAL, dec√≠ claramente que no ten√©s esa informaci√≥n
6. Respond√© de forma clara, completa y amable
7. Compart√≠ los enlaces que aparezcan en el CONTEXTO cuando sean relevantes

FORMATO DE RESPUESTA:
- Us√° formato Markdown para estructurar tu respuesta
- IMPORTANTE: Despu√©s de cada t√≠tulo y secci√≥n, us√° DOS saltos de l√≠nea (presion√° Enter dos veces)
- Comenz√° con un t√≠tulo en negrita: **Nombre del Tr√°mite** seguido de dos saltos de l√≠nea
- Us√° emojis relevantes para hacer la respuesta m√°s amigable (üìã, üíª, üè•, üì±, etc.)
- Organiz√° la informaci√≥n con subt√≠tulos en negrita seguidos de dos saltos de l√≠nea
- Us√° listas con guiones (-) para documentos o pasos
- Los enlaces deben estar en formato markdown: [texto del enlace](URL)
- Cada secci√≥n debe estar separada con l√≠neas vac√≠as

IMPORTANTE SOBRE ENLACES:
- Solo inclu√≠ enlaces que est√©n EXPL√çCITAMENTE en el CONTEXTO DEL TR√ÅMITE
- NO inventes enlaces ni URLs
- Si el contexto tiene enlaces, incluilos en formato markdown al final de la respuesta
- Si NO hay enlaces en el contexto, NO incluyas ning√∫n enlace

CONTEXTO DEL TR√ÅMITE ACTUAL (ESTA ES TU √öNICA FUENTE DE INFORMACI√ìN):
{contexto}"""

    if historial:
        system_prompt += f"\n\nPara referencia, aqu√≠ est√° el historial de la conversaci√≥n (solo √∫salo si es necesario para entender mejor la pregunta):\n{historial}"
    
    system_prompt += "\n\nResponde la siguiente consulta bas√°ndote √öNICAMENTE en el CONTEXTO DEL TR√ÅMITE ACTUAL de arriba:"

    prompt_final = f"{system_prompt}\n\nUsuario: {consulta}\n\nAsistente:"
    
    return prompt_final

async def llamar_ollama(prompt: str) -> str:
    try:
        async with httpx.AsyncClient() as client:
            response = await client.post(
                OLLAMA_URL,
                json={
                    "model": OLLAMA_MODEL,
                    "prompt": prompt,
                    "stream": False
                },
                timeout=1000.0
            )
            
            if response.status_code != 200:
                return "Error al comunicarse con el asistente de IA. Por favor, intent√° nuevamente."
            
            resultado = response.json()
            respuesta = resultado.get("response", "")
            
            return respuesta
            
    except httpx.TimeoutException:
        return "El asistente est√° tardando mucho en responder. Por favor, intent√° nuevamente."
    except Exception as e:
        print(f"‚ùå Error en llamar_ollama: {e}")
        return "Ocurri√≥ un error al procesar tu consulta. Por favor, intent√° nuevamente."

async def generar_respuesta_con_rag(
    consulta: str, 
    nombre_usuario: str,
    historial: str = ""
) -> str:
    """
    Funci√≥n principal del RAG: busca contexto relevante y genera respuesta
    
    Args:
        consulta: Pregunta del usuario
        nombre_usuario: Nombre del usuario para personalizar
        historial: Historial de conversaci√≥n previo (opcional)
    
    Returns:
        str: Respuesta generada con contexto o mensaje de no disponibilidad
    """
    tramites = search_tramites(consulta, n_results=1)
    
    if not tramites:
        return f"Hola {nombre_usuario}, no tengo informaci√≥n espec√≠fica sobre ese tr√°mite en mi base de datos. Te recomiendo contactar directamente a PAMI al 138 o visitar https://www.pami.org.ar para m√°s informaci√≥n."
    
    tramite = tramites[0]
    contexto = formatear_tramite_como_texto(tramite)
    
    prompt = construir_prompt_con_contexto(consulta, contexto, nombre_usuario, historial)
    
    respuesta = await llamar_ollama(prompt)
    
    return respuesta